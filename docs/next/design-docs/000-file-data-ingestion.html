<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>000 - File Data Ingestion · Hazelcast Jet</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="Unified API for reading files and improved packaging for cloud sources"/><meta name="docsearch:version" content="next"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="000 - File Data Ingestion · Hazelcast Jet"/><meta property="og:type" content="website"/><meta property="og:url" content="https://jet-start.sh/"/><meta property="og:description" content="Unified API for reading files and improved packaging for cloud sources"/><meta property="og:image" content="https://jet-start.sh/img/Hazelcast-Jet-Logo-Blue_Dark.jpg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://jet-start.sh/img/Hazelcast-Jet-Logo-Blue_Dark.jpg"/><link rel="shortcut icon" href="/img/favicon.png"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.css"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><link rel="alternate" type="application/atom+xml" href="https://jet-start.sh/blog/atom.xml" title="Hazelcast Jet Blog ATOM Feed"/><link rel="alternate" type="application/rss+xml" href="https://jet-start.sh/blog/feed.xml" title="Hazelcast Jet Blog RSS Feed"/><script>
              (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
              (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
              m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
              })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

              ga('create', 'UA-158279495-1', 'auto');
              ga('send', 'pageview');
            </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,500,600"/><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,500,600,700,800"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script><script type="text/javascript" src="/js/code-block-buttons.js"></script><script type="text/javascript" src="https://plausible.io/js/plausible.js" async="" defer="" data-domain="jet-start.sh"></script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/prism.css"/><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/logo-dark.svg" alt="Hazelcast Jet"/></a><a href="/versions"><h3>next</h3></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/docs/next/get-started/intro" target="_self">Docs</a></li><li class=""><a href="/download" target="_self">Download</a></li><li class=""><a href="/demos" target="_self">Demos</a></li><li class=""><a href="https://github.com/hazelcast/hazelcast-jet" target="_self">GitHub</a></li><li class=""><a href="https://slack.hazelcast.com/" target="_self">Community</a></li><li class=""><a href="/blog/" target="_self">Blog</a></li><li class="navSearchWrapper reactNavSearchWrapper"><input type="text" id="search_input_react" placeholder="Search" title="Search"/></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="container mainContainer docsContainer"><div class="wrapper"><div class="post"><header class="postHeader"><a class="edit-page-link button" href="https://github.com/hazelcast/hazelcast-jet/edit/master/site/docs/design-docs/000-file-data-ingestion.md" target="_blank" rel="noreferrer noopener">Edit</a><h1 id="__docusaurus" class="postHeaderTitle">000 - File Data Ingestion</h1></header><article><div><span><p><em>Since 4.4.</em></p>
<p>= 2. Support for common formats and data sources</p>
<p>Format and source of the data are seemingly orthogonal. You can read
plain text files/csv/avro/.. etc from a local filesystem, S3 etc.</p>
<p>However, to take an advantage of some properties of certain formats
(e.g. parallel reading/deserialization of Avro files, selecting columns
in parquet …) requires combining these two steps (read Avro file header
to find out the beginning positions of data blocks).</p>
<p>This is already implemented in the Hadoop connector.</p>
<p>Required Formats</p>
<ul>
<li><p>CSV
We don’t have a connector
There is not official Hadoop connector (= InputFormat), open-source
implementations exist, with limitations (same as with Spark)</p></li>
<li><p>JSON
We have a connector
There is not official Hadoop connector (= InputFormat), open-source
implementations exist</p></li>
<li><p>Avro
We have a connector
There is official Hadoop connector</p></li>
<li><p>Parquet
We don’t have a connector
There is official Hadoop connector</p></li>
<li><p>ORC low priority</p></li>
</ul>
<p>** <a href="https://orc.apache.org/docs/core-java.html#reading-orc-files">Reading ORC files</a></p>
<p>== Parquet</p>
<p>Parquet is a special case between the formats listed above - it defines
representation in the file (as others do), but doesn't have own schema
(class?) definition for deserialization into objects. Uses one of the
other serialization formats, most commonly thrift, avro.</p>
<p>The PRD doesn't specify the following commonly used formats:</p>
<ul>
<li><p>plain text,</p></li>
<li><p>binary (e.g. images for ML)</p></li>
<li><p>Protobuf</p></li>
<li><p>Thrift</p></li>
</ul>
<p>Sources</p>
<ul>
<li><p>local filesystem</p></li>
<li><p>Amazon S3</p></li>
<li><p>Azure Blob Storage</p></li>
<li><p>Azure Data Lake Storage</p></li>
<li><p>Google Cloud Storage.</p></li>
<li><p>HDFS ? not listed in the PRD</p></li>
</ul>
<p>== Unified approach for file data ingestion</p>
<p>We have the following possibilities</p>
<ol>
<li><p>enforce naming convention, e.g.
<code>com.hazelcast.jet.x.XSources.x(...., formatY)</code>
<code>com.hazelcast.jet.s3.S3Sources.s3(..., Avro)</code></p></li>
<li><p>new File API as a single entry point e.g.
<code>FileSources.s3(&amp;quot;...&amp;quot;).withFormat(AVRO)</code></p></li>
<li><p>URL/ resource description style with auto detection, e.g.
<code>FileSource.read(&amp;quot;s3://...../file.avro&amp;quot;);</code></p></li>
</ol>
<p>We think we should go with 2., potentially write a parser for 3. to
delegate to 2. - would be used from SQL (there already is similar logic)</p>
<p>We decided to use Hadoop libraries to access all systems apart from
local filesystem, and Hadoop can detect the source we implemented 3.</p>
<p>== Using Hadoop libraries</p>
<p>Using Hadoop libraries is a must for reading data from HDFS.</p>
<p>To read data from other sources, like S3 it is possible to use custom
connectors (e.g. we have a connector for S3). Both approaches have some
advantages and disadvantages.</p>
<p>Using Hadoop libs</p>
<ul>
<li><p><code>-</code> Complicated to setup, lot’s of dependencies, possible dependency
conflicts.</p></li>
<li><p><code>+</code> Supports advanced features - can take advantage of the structure
of the formats - e.g. read avro file in parallel, or read specific
columns in parquet.  +-? The hadoop api leaks (see
<code>com.hazelcast.jet.hadoop.HadoopSources#inputFormat(org.apache.hadoop.conf.Configuration, com.hazelcast.function.BiFunctionEx&lt;K,V,E&gt;</code>) It is questionable if this
is an issue, exposing the hadoop API gives users more power.  For
performance difference see the benchmark below</p></li>
</ul>
<p>Using specific s3/... connector</p>
<ul>
<li><p><code>-</code> we would need to implement a connector for each source (currently we
have local filesystem and S3), the S3 source connector is ~300 lines</p></li>
<li><p><code>-</code> we would miss the advanced features, reimplementing those would be
a huge effort</p></li>
<li><p><code>+</code> Simpler packaging</p></li>
<li><p><code>+</code> Nicer API</p></li>
<li><p><code>+-</code>? Unknown performance compared to Hadoop libs</p></li>
</ul>
<p>Benchmark S3 client based vs Hadoop based connectors</p>
<p>Benchmark summary: S3 is generally faster, apart from 1 large file case,
where Hadoop can split the file. The difference is not massive though
(18211 fastest for s3, vs 21306 for Hadoop, slowest 75752 for S3, 90071
for Hadoop)</p>
<p>We decided to use Hadoop to access all sources, with option without
hadoop for local filesystems</p>
<p>= 1. Loading data from a file Cookbook</p>
<p>There is a section in the manual describing the new API and each module.</p>
<p>There are examples how to read:</p>
<ul>
<li>binary files</li>
<li>text files by lines</li>
<li>avro files</li>
</ul>
<p>= 3. Any other concerns?</p>
<p>Compression</p>
<p>Hadoop supports various compression formats.</p>
<p>= 4. Overlap with Jet SQL</p>
<p>The main difference in how SQL would use the connector is that it
expects <code>Object[]</code> as return type.</p>
<p>We need to provide a way to configure each format as such.</p>
<p>= Design</p>
<p>Entry point - <code>com.hazelcast.jet.pipeline.file.FileSources</code>. Returns
<code>com.hazelcast.jet.pipeline.file.FileSourceBuilder</code></p>
<p>There are 2 required parameters - path and format.</p>
<p>Path specifies the file location (accepts globs).
The format describes the format of the data in the file (text, csv,
json ..).</p>
<p>The files are either on a local filesystem, hdfs, or on one of
supported cloud storage systems, implemented using Hadoop
infrastructure.</p>
<p>User must provide correct module on CP.</p>
<p>Additionally, local files can be read using Hadoop infrastructure by
setting <code>useHadoopForLocalFiles</code> flag on the builder.</p>
<p>It is also possible to pass key/value String pairs as options. These
are passed to the Hadoop MR job configuration - needed for
authentication and available to use for any options for the format or
other needs.</p>
<p>== Local files</p>
<p>Reading from local file system is implemented in
<code>com.hazelcast.jet.pipeline.file.impl.LocalFileSourceFactory</code>.
The current infrastructure is reused -
<code>com.hazelcast.jet.core.processor.SourceProcessors.readFilesP (java.lang.String, java.lang.String, boolean, com.hazelcast.function.FunctionEx&lt;? super java.nio.file.Path,? extends java.util.stream.Stream&lt;I&gt;&gt;)</code></p>
<p>Supporting a file format in LocalFileSourceFactory means implementing a
<code>ReadFileFnProvider</code> interface:</p>
<pre><code class="hljs css language-java"><span class="token keyword">public</span> <span class="token keyword">interface</span> <span class="token class-name">ReadFileFnProvider</span> <span class="token punctuation">{</span>

    <span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">T</span><span class="token punctuation">></span></span> <span class="token class-name">FunctionEx</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Path</span><span class="token punctuation">,</span> <span class="token class-name">Stream</span><span class="token punctuation">&lt;</span><span class="token class-name">T</span><span class="token punctuation">></span><span class="token punctuation">></span></span> <span class="token function">createReadFileFn</span><span class="token punctuation">(</span><span class="token class-name">FileFormat</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">T</span><span class="token punctuation">></span></span> format<span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token class-name">String</span> <span class="token function">format</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre>
<p>The <code>createReadFileFn</code> creates, for a given file format, a function
that reads from a Path (a file on a local filesystem) and returns
a stream of items, which are emitted from the source.</p>
<p>== Cloud</p>
<p>Cloud storage systems are supported via Hadoop. Each storage system is
supported by a given module, which includes all dependencies.
The concrete storage is detected from the path prefix by the Hadoop
infrastructure.</p>
<p>Supporting a file format in HadoopFileSourceFactory means implementing
<code>JobConfigurer</code> interface:</p>
<pre><code class="hljs css language-java"><span class="token keyword">public</span> <span class="token keyword">interface</span> <span class="token class-name">JobConfigurer</span> <span class="token punctuation">{</span>

    <span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">T</span><span class="token punctuation">></span></span> <span class="token keyword">void</span> <span class="token function">configure</span><span class="token punctuation">(</span><span class="token class-name">Job</span> job<span class="token punctuation">,</span> <span class="token class-name">FileFormat</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">T</span><span class="token punctuation">></span></span> format<span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token class-name">BiFunctionEx</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token operator">?</span><span class="token punctuation">,</span> <span class="token operator">?</span><span class="token punctuation">,</span> <span class="token operator">?</span><span class="token punctuation">></span></span> <span class="token function">projectionFn</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre>
<p>The <code>configure</code> configured the MR job with given <code>FileFormat</code>. This
typically means setting the InputFormat class for the MR job and its
parameters.</p>
<p>The <code>projectionFn</code> function converts <code>InputFormat</code>'s key-value result
to the item emitted from the source.</p>
<p>== Testing</p>
<p>Tests for each format are part of the Hadoop module, which runs them in
two modes:</p>
<ul>
<li><p>local mode</p></li>
<li><p>using Hadoop local filesystem</p></li>
</ul>
<p>This means that the tests for the local filesystem are not part of the
hazelcast-jet-core module where the implementation is.
On the other hand we reuse the same set of tests, so we have the same
coverage for both implementations.</p>
<p>Integrations tests are part of the hazelcast-qe pipeline, where the jobs
run in a cluster inside a docker container, with the connector fat jars.
This ensures that the fat jars contain correct dependencies.</p>
<p>= Licensing</p>
<p>We had to add couple of aliases for apache 2, BSD, new/revised BSD,
nothing new</p>
<p>What's new is:</p>
<ul>
<li><p>&quot;The Go license&quot; - this is permissive BSD style license,
<a href="https://golang.org/LICENSE">link</a></p></li>
<li><p>CDDL (1.0) - Common Development and Distribution License - this is
a weak copyleft license, based on mozilla public license and its
variants</p></li>
</ul>
<p>** CDDL 1.1 (minor update, something with patents and EU law)
** CDDL + GPLv2 with classpath exception - this is just dual CDDL
1.0 + GPLv2 license</p>
<p>All the CDDL libs are transitive dependencies of hadoop-common, which
we plan to package in the all deps included file connectors jars for
s3/gcp/azure.</p>
<p>Many commercial applications use CDDL dependencies (e.g. Spring
framework has many modules with transitive dependencies under CDDL)</p>
</span></div></article></div><div class="docs-prevnext"></div></div></div><nav class="onPageNav"></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><div style="text-align:left"><a href="/" class="nav-home"><img src="/img/logo-light.svg" alt="Hazelcast Jet" width="200" height="40"/></a><div style="margin-left:12px"><a class="github-button" href="https://github.com/hazelcast/hazelcast-jet" data-icon="octicon-star" data-count-href="/facebook/docusaurus/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star On GitHub</a></div></div><div><h5>Docs</h5><a href="/docs/get-started/intro">Get Started</a><a href="/docs/concepts/dag">Concepts</a><a href="/docs/tutorials/kafka">Tutorials</a><a href="/docs/architecture/distributed-computing">Architecture</a><a href="/docs/operations/installation">Operations Guide</a><a href="/docs/enterprise">Enterprise Edition</a></div><div><h5>Community</h5><a href="https://groups.google.com/forum/#!forum/hazelcast-jet" target="_blank" rel="noreferrer noopener">Google Groups</a><a href="http://stackoverflow.com/questions/tagged/hazelcast-jet" target="_blank" rel="noreferrer noopener">Stack Overflow</a><a href="https://slack.hazelcast.com">Slack</a></div><div><h5>Latest From the Blog</h5><a href="/blog/2020/10/23/jet-43-is-released">Jet 4.3 Released</a><a href="/blog/2020/10/06/enabling-full-text-search">Enabling Full-text Search with Change Data Capture in a Legacy Application</a><a href="/blog/2020/09/18/cdc-meets-stream-processing">Change Data Capture meets Stream Processing</a><a href="/blog/2020/08/05/gc-tuning-for-jet">Sub-10 ms Latency in Java: Concurrent GC with Green Threads</a><a href="/blog/2020/07/16/designing-evergreen-cache-cdc">Designing an Evergreen Cache with Change Data Capture</a></div><div><h5>More</h5><a href="https://github.com/hazelcast/hazelcast-jet">GitHub Project</a><a href="http://hazelcast.com/company/careers/">Work at Hazelcast</a><a href="/license">License</a></div></section><section class="copyright">Copyright © 2020 Hazelcast Inc.</section></footer></div><script type="text/javascript" src="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.js"></script><script>
                document.addEventListener('keyup', function(e) {
                  if (e.target !== document.body) {
                    return;
                  }
                  // keyCode for '/' (slash)
                  if (e.keyCode === 191) {
                    const search = document.getElementById('search_input_react');
                    search && search.focus();
                  }
                });
              </script><script>
              var search = docsearch({
                
                apiKey: '79d1e4941621b9fd761d279d4d19ed69',
                indexName: 'hazelcast-jet',
                inputSelector: '#search_input_react',
                algoliaOptions: {"facetFilters":["language:en","version:next"]}
              });
            </script></body></html>